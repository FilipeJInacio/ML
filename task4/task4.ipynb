{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de10c52e",
   "metadata": {},
   "source": [
    "### **Imports Made**:\n",
    "- Numpy for general purpose tasks (ex: file loads, matrix operations and arrangements, etc)\n",
    "- TensorFlow for the entire CNN structure and implementation, as well as a multitude of additional general purpose functions regarding machine learning and transfer learning models used like Inception V3\n",
    "- sklearn.model_selection for the train test split function that allows our data split\n",
    "- sklearn.linear_metrics for more precise accuracy indicators, and confusion matrix representation\n",
    "- imlearn.over_sampling for the SMOTE data augmentation function (commented because it provided worse results than regular data augmentation)\n",
    "- Random Set Seeds to created a deterministic environment for weight assignements and random calculations\n",
    "- Keras Tuner for CNN hyperparameter determination, was especially useful in the Dropout layers values calculations\n",
    "\n",
    "The key metric to be evaluated will be Balanced Accuracy, just as stated in the project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14239f-07ac-401a-8470-15199bfe188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import shuffle\n",
    "from numpy import random\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "tf.random.set_seed(57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1db10666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10629, 2352)\n",
      "(10629,)\n",
      "(2658, 2352)\n",
      "Class 1 is 50.44689058236899% of the data set\n",
      "Class 2 is 8.373318280176875% of the data set\n",
      "Class 3 is 1.0913538432590084% of the data set\n",
      "Class 4 is 21.685953523379435% of the data set\n",
      "Class 5 is 9.314140558848434% of the data set\n",
      "Class 6 is 9.08834321196726% of the data set\n",
      "(10629, 28, 28, 3)\n",
      "Class 1 is 50.44102081618252% of the train data set\n",
      "Class 2 is 8.373515229918853% of the train data set\n",
      "Class 3 is 1.0937316241326591% of the train data set\n",
      "Class 4 is 21.68646360108197% of the train data set\n",
      "Class 5 is 9.314359637774903% of the train data set\n",
      "Class 6 is 9.090909090909092% of the train data set \n",
      "\n",
      "Class 1 is 50.47036688617121% of the validation data set\n",
      "Class 2 is 8.372530573847602% of the validation data set\n",
      "Class 3 is 1.0818438381937912% of the validation data set\n",
      "Class 4 is 21.683913452492945% of the validation data set\n",
      "Class 5 is 9.313264346190028% of the validation data set\n",
      "Class 6 is 9.078080903104421% of the validation data set\n"
     ]
    }
   ],
   "source": [
    "augmented_class2 = []\n",
    "augmented_class3 = []\n",
    "augmented_class4 = []\n",
    "augmented_class5 = []\n",
    "augmented_class6 = []\n",
    "\n",
    "x = np.load(\"Xtrain_Classification2.npy\")\n",
    "y = np.load(\"ytrain_Classification2.npy\")\n",
    "x_test = np.load(\"Xtest_Classification2.npy\")\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "\n",
    "n_class1 = len([0 for i in range(y.shape[0]) if y[i] == 0])\n",
    "n_class2 = len([0 for i in range(y.shape[0]) if y[i] == 1])\n",
    "n_class3 = len([0 for i in range(y.shape[0]) if y[i] == 2])\n",
    "n_class4 = len([0 for i in range(y.shape[0]) if y[i] == 3])\n",
    "n_class5 = len([0 for i in range(y.shape[0]) if y[i] == 4])\n",
    "n_class6 = len([0 for i in range(y.shape[0]) if y[i] == 5])\n",
    "\n",
    "print(f'Class 1 is {(n_class1/(n_class1 + n_class2 + n_class3 + n_class4 + n_class5 + n_class6)) * 100}% of the data set')\n",
    "print(f'Class 2 is {(n_class2/(n_class1 + n_class2 + n_class3 + n_class4 + n_class5 + n_class6)) * 100}% of the data set')\n",
    "print(f'Class 3 is {(n_class3/(n_class1 + n_class2 + n_class3 + n_class4 + n_class5 + n_class6)) * 100}% of the data set')\n",
    "print(f'Class 4 is {(n_class4/(n_class1 + n_class2 + n_class3 + n_class4 + n_class5 + n_class6)) * 100}% of the data set')\n",
    "print(f'Class 5 is {(n_class5/(n_class1 + n_class2 + n_class3 + n_class4 + n_class5 + n_class6)) * 100}% of the data set')\n",
    "print(f'Class 6 is {(n_class6/(n_class1 + n_class2 + n_class3 + n_class4 + n_class5 + n_class6)) * 100}% of the data set')\n",
    "\n",
    "x = np.reshape(x,(len(x),28,28,3))\n",
    "\n",
    "x = (x).astype('float32')/255.0\n",
    "\n",
    "x_test = np.reshape(x_test,(len(x_test),28,28,3))\n",
    "\n",
    "x_test = (x_test).astype('float32')/255.0\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(x, y, stratify = y,test_size = 0.2)\n",
    "\n",
    "n_class1t = len([0 for i in range(y_train.shape[0]) if y_train[i] == 0])\n",
    "n_class2t = len([0 for i in range(y_train.shape[0]) if y_train[i] == 1])\n",
    "n_class3t = len([0 for i in range(y_train.shape[0]) if y_train[i] == 2])\n",
    "n_class4t = len([0 for i in range(y_train.shape[0]) if y_train[i] == 3])\n",
    "n_class5t = len([0 for i in range(y_train.shape[0]) if y_train[i] == 4])\n",
    "n_class6t = len([0 for i in range(y_train.shape[0]) if y_train[i] == 5])\n",
    "\n",
    "print(f'Class 1 is {(n_class1t/(n_class1t + n_class2t + n_class3t + n_class4t + n_class5t + n_class6t)) * 100}% of the train data set')\n",
    "print(f'Class 2 is {(n_class2t/(n_class1t + n_class2t + n_class3t + n_class4t + n_class5t + n_class6t)) * 100}% of the train data set')\n",
    "print(f'Class 3 is {(n_class3t/(n_class1t + n_class2t + n_class3t + n_class4t + n_class5t + n_class6t)) * 100}% of the train data set')\n",
    "print(f'Class 4 is {(n_class4t/(n_class1t + n_class2t + n_class3t + n_class4t + n_class5t + n_class6t)) * 100}% of the train data set')\n",
    "print(f'Class 5 is {(n_class5t/(n_class1t + n_class2t + n_class3t + n_class4t + n_class5t + n_class6t)) * 100}% of the train data set')\n",
    "print(f'Class 6 is {(n_class6t/(n_class1t + n_class2t + n_class3t + n_class4t + n_class5t + n_class6t)) * 100}% of the train data set \\n')\n",
    "\n",
    "n_class1v = len([0 for i in range(y_val.shape[0]) if y_val[i] == 0])\n",
    "n_class2v = len([0 for i in range(y_val.shape[0]) if y_val[i] == 1])\n",
    "n_class3v = len([0 for i in range(y_val.shape[0]) if y_val[i] == 2])\n",
    "n_class4v = len([0 for i in range(y_val.shape[0]) if y_val[i] == 3])\n",
    "n_class5v = len([0 for i in range(y_val.shape[0]) if y_val[i] == 4])\n",
    "n_class6v = len([0 for i in range(y_val.shape[0]) if y_val[i] == 5])\n",
    "\n",
    "print(f'Class 1 is {(n_class1v/(n_class1v + n_class2v + n_class3v + n_class4v + n_class5v + n_class6v)) * 100}% of the validation data set')\n",
    "print(f'Class 2 is {(n_class2v/(n_class1v + n_class2v + n_class3v + n_class4v + n_class5v + n_class6v)) * 100}% of the validation data set')\n",
    "print(f'Class 3 is {(n_class3v/(n_class1v + n_class2v + n_class3v + n_class4v + n_class5v + n_class6v)) * 100}% of the validation data set')\n",
    "print(f'Class 4 is {(n_class4v/(n_class1v + n_class2v + n_class3v + n_class4v + n_class5v + n_class6v)) * 100}% of the validation data set')\n",
    "print(f'Class 5 is {(n_class5v/(n_class1v + n_class2v + n_class3v + n_class4v + n_class5v + n_class6v)) * 100}% of the validation data set')\n",
    "print(f'Class 6 is {(n_class6v/(n_class1v + n_class2v + n_class3v + n_class4v + n_class5v + n_class6v)) * 100}% of the validation data set')\n",
    "\n",
    "class1 = X_train[y_train == 0]\n",
    "class2 = X_train[y_train == 1]\n",
    "class3 = X_train[y_train == 2]\n",
    "class4 = X_train[y_train == 3]\n",
    "class5 = X_train[y_train == 4]\n",
    "class6 = X_train[y_train == 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f182d75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  tf.keras.layers.RandomRotation(0.5),\n",
    "])\n",
    "\n",
    "for i in range(n_class1t - n_class2t):\n",
    "    random_index = np.random.randint(0, n_class2t)\n",
    "    augmented_image = data_augmentation(tf.expand_dims(class2[random_index], 0))\n",
    "    augmented_class2.append(augmented_image)\n",
    "\n",
    "augmented_class2 = np.vstack(augmented_class2)\n",
    "\n",
    "X_train_balanced = np.vstack([X_train, augmented_class2])\n",
    "y_train_balanced = np.concatenate([y_train, np.ones(n_class1t - n_class2t)])\n",
    "\n",
    "for i in range(n_class1t - n_class3t):\n",
    "    random_index = np.random.randint(0, n_class3t)\n",
    "    augmented_image = data_augmentation(tf.expand_dims(class3[random_index], 0))\n",
    "    augmented_class3.append(augmented_image)\n",
    "\n",
    "augmented_class3 = np.vstack(augmented_class3)\n",
    "\n",
    "X_train_balanced = np.vstack([X_train_balanced, augmented_class3])\n",
    "y_train_balanced = np.concatenate([y_train_balanced, 2*np.ones(n_class1t - n_class3t)])\n",
    "\n",
    "for i in range(n_class1t - n_class4t):\n",
    "    random_index = np.random.randint(0, n_class4t)\n",
    "    augmented_image = data_augmentation(tf.expand_dims(class4[random_index], 0))\n",
    "    augmented_class4.append(augmented_image)\n",
    "\n",
    "augmented_class4 = np.vstack(augmented_class4)\n",
    "\n",
    "X_train_balanced = np.vstack([X_train_balanced, augmented_class4])\n",
    "y_train_balanced = np.concatenate([y_train_balanced, 3*np.ones(n_class1t - n_class4t)])\n",
    "\n",
    "for i in range(n_class1t - n_class5t):\n",
    "    random_index = np.random.randint(0, n_class5t)\n",
    "    augmented_image = data_augmentation(tf.expand_dims(class5[random_index], 0))\n",
    "    augmented_class5.append(augmented_image)\n",
    "\n",
    "augmented_class5 = np.vstack(augmented_class5)\n",
    "\n",
    "X_train_balanced = np.vstack([X_train_balanced, augmented_class5])\n",
    "y_train_balanced = np.concatenate([y_train_balanced, 4*np.ones(n_class1t - n_class5t)])\n",
    "\n",
    "for i in range(n_class1t - n_class6t):\n",
    "    random_index = np.random.randint(0, n_class6t)\n",
    "    augmented_image = data_augmentation(tf.expand_dims(class6[random_index], 0))\n",
    "    augmented_class6.append(augmented_image)\n",
    "\n",
    "augmented_class6 = np.vstack(augmented_class6)\n",
    "\n",
    "X_train_balanced = np.vstack([X_train_balanced, augmented_class6])\n",
    "y_train_balanced = np.concatenate([y_train_balanced, 5*np.ones(n_class1t - n_class6t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97657820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 is 16.666666666666664% of the balanced data set\n",
      "Class 2 is 16.666666666666664% of the balanced data set\n",
      "Class 3 is 16.666666666666664% of the balanced data set\n",
      "Class 4 is 16.666666666666664% of the balanced data set\n",
      "Class 5 is 16.666666666666664% of the balanced data set\n",
      "Class 6 is 16.666666666666664% of the balanced data set\n"
     ]
    }
   ],
   "source": [
    "n_class1a = len([0 for i in range(y_train_balanced.shape[0]) if y_train_balanced[i] == 0])\n",
    "n_class2a = len([0 for i in range(y_train_balanced.shape[0]) if y_train_balanced[i] == 1])\n",
    "n_class3a = len([0 for i in range(y_train_balanced.shape[0]) if y_train_balanced[i] == 2])\n",
    "n_class4a = len([0 for i in range(y_train_balanced.shape[0]) if y_train_balanced[i] == 3])\n",
    "n_class5a = len([0 for i in range(y_train_balanced.shape[0]) if y_train_balanced[i] == 4])\n",
    "n_class6a = len([0 for i in range(y_train_balanced.shape[0]) if y_train_balanced[i] == 5])\n",
    "\n",
    "print(f'Class 1 is {(n_class1a/(n_class1a + n_class2a + n_class3a + n_class4a + n_class5a + n_class6a)) * 100}% of the balanced data set')\n",
    "print(f'Class 2 is {(n_class2a/(n_class1a + n_class2a + n_class3a + n_class4a + n_class5a + n_class6a)) * 100}% of the balanced data set')\n",
    "print(f'Class 3 is {(n_class3a/(n_class1a + n_class2a + n_class3a + n_class4a + n_class5a + n_class6a)) * 100}% of the balanced data set')\n",
    "print(f'Class 4 is {(n_class4a/(n_class1a + n_class2a + n_class3a + n_class4a + n_class5a + n_class6a)) * 100}% of the balanced data set')\n",
    "print(f'Class 5 is {(n_class5a/(n_class1a + n_class2a + n_class3a + n_class4a + n_class5a + n_class6a)) * 100}% of the balanced data set')\n",
    "print(f'Class 6 is {(n_class6a/(n_class1a + n_class2a + n_class3a + n_class4a + n_class5a + n_class6a)) * 100}% of the balanced data set')\n",
    "\n",
    "X_train_balanced_shuffled, y_train_balanced_shuffled = shuffle(X_train_balanced, y_train_balanced, random_state=0)\n",
    "\n",
    "y_train_balanced_c = tf.keras.utils.to_categorical(y_train_balanced_shuffled,6)\n",
    "y_val_c = tf.keras.utils.to_categorical(y_val,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afe8d3",
   "metadata": {},
   "source": [
    "## **Convolutional Neural Network**:\n",
    "\n",
    "The model consists of a sequence of initial convolutional layers, especialized to provide and recongnize numerous patterns and features within the image, and this information is then subsequently processed and repurposed through our use of Dropout layers in between each MaxPooling2D, and finally a BatchNormalization layer in the MLP section followed by a Dropout layer of considerable intesity (0.7). Throughout testing with the CNN hyperparameter tuning this model proved to be the one most efficient in providing the least validation loss in our data, along with a relatively high batch size of 1000 that allows the stabilization of validation loss improvement while training. 100 epochs allows (in the graph plotted in the end of the section) full view of validation loss behaviours during the training process, most notably allowing the observation of plateau zones. This added with an early stopping callback implemented to return to the weights of the sequence with the lowest validation loss provides our training with a very thorough and careful approach to determine the best possible weight configuration. \n",
    "\n",
    "Finally after the best weight structure is assigned, the balanced accuracy of the validation set is externally calculated and it's confusion matrix is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a62c4af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/user/AAut/ML/task4/task4.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m lr \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model_MLP \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mSequential([\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mConv2D(\u001b[39m32\u001b[39m, kernel_size\u001b[39m=\u001b[39m(\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m,input_shape\u001b[39m=\u001b[39m(\u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m, \u001b[39m3\u001b[39m)),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mConv2D(\u001b[39m32\u001b[39m, kernel_size\u001b[39m=\u001b[39m(\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mConv2D(\u001b[39m32\u001b[39m, kernel_size\u001b[39m=\u001b[39m(\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mMaxPooling2D(pool_size\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m),strides\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDropout(\u001b[39m0.2\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mConv2D(\u001b[39m64\u001b[39m, kernel_size\u001b[39m=\u001b[39m(\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mMaxPooling2D(pool_size\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m),strides\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDropout(\u001b[39m0.3\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mConv2D(\u001b[39m128\u001b[39m, kernel_size\u001b[39m=\u001b[39m(\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mMaxPooling2D(pool_size\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m),strides\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDropout(\u001b[39m0.3\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mFlatten(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m64\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mBatchNormalization(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDropout(\u001b[39m0.7\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m2\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m ])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m model_MLP\u001b[39m.\u001b[39msummary()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/AAut/ML/task4/task4.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m adam \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate \u001b[39m=\u001b[39m lr)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 1000\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "\n",
    "model_MLP = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu',input_shape=(28, 28, 3)),\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=2),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=2),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=2),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "model_MLP.summary()\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "model_MLP.compile(optimizer = adam,\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "early_callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 20, verbose = 1, restore_best_weights = True)\n",
    "\n",
    "history = model_MLP.fit(x = X_train_balanced_shuffled, y = y_train_balanced_c, epochs = epochs, batch_size = batch_size, validation_data = (X_val,y_val_c), callbacks = [early_callback])\n",
    "\n",
    "y_pred = model_MLP.predict(X_val)\n",
    "y_pred = np.argmax(y_pred, axis = 1)\n",
    "model_MLP.evaluate(X_val, y_val_c)\n",
    "y_true = np.argmax(y_val_c, axis = 1)\n",
    "print(f'Balanced Accuracy score is {balanced_accuracy_score(y_true, y_pred)}')\n",
    "\n",
    "\n",
    "####PLOT EVOLUTION\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.plot(history.history['loss'], label = 'train', color = \"blue\", linewidth = 3, alpha = 0.6)\n",
    "plt.plot(history.history['val_loss'], label = 'validation', color = \"green\", linewidth = 3, alpha = 0.6)\n",
    "plt.xlabel(r\"Epochs\")\n",
    "plt.ylabel(r\"Loss\")\n",
    "plt.title(r\"CNN Evaluation\", fontsize = 14)\n",
    "plt.legend(labels=[r'Train', r'Validation'])\n",
    "plt.grid(linestyle = '--', alpha = 0.5)\n",
    "plt.show()\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d91d140",
   "metadata": {},
   "source": [
    "### **Test Prediction and Submission**:\n",
    "\n",
    "This section includes the final test set prediction and save of the submission y_test file, as well as saving it's loss graph for later implementation for a Matlab graph (as seen in the final report)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb74e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test)\n",
    "print(x_test.shape)\n",
    "\n",
    "y_test = model_MLP.predict(x_test)\n",
    "y_test = np.argmax(y_test, axis = 1)\n",
    "\n",
    "\n",
    "print(y_test.shape)\n",
    "print(y.shape)\n",
    "\n",
    "np.save(\"ytest_Classification2.npy\", y_test)\n",
    "np.savetxt('x.txt', history.history['loss'], fmt='%f')\n",
    "np.savetxt('y.txt', history.history['val_loss'], fmt='%f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
